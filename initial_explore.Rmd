---
title: "Ask Abby Analysis"
output:
  html_document:
    theme: readable
    df_print: paged
---

# Setup

```{r setup, results='hide'}
pacman::p_load(tidyverse, tidytext, feather)
source("theme_lanyop.R")
```

The data is scraped from historic 'Dear Abby' column letters.  It's currently on GitHub.  This is how you'd get it straight from the source.
```{r data-pull, eval=FALSE}
data_url <- "https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv"

# Specify column types
da_data <- read_csv(data_url, col_types = list(col_double(),
                                               col_double(),
                                               col_character(),
                                               col_character(),
                                               col_character(),
                                               col_double(),
                                               col_character()))

# Parsing issue fix:  day (one row has extra ,)
da_data <- da_data %>% 
  mutate(day = str_remove(day, ",") %>% as.numeric(.),
         decade = year - (year %% 10))  # Also build decade cat

head(da_data)
```

# Check Distributions

### Numerics:
Handle these like numeric categoricals
```{r num-dist, results='hold', warning=FALSE, echo=FALSE}
# Identify numeric cols
num_vars <- da_data %>% select_if(is.numeric) %>% names

# Basic distribution plotting helper
dist_plot <- function(df, x){
  
  x <- enquo(x)
  
  ggplot(df) + 
    geom_histogram(aes(!!x), binwidth = 1) +
    theme_lanyop()
}

# Build plots in loop
plots <- da_data %>% 
  map_if(is.numeric, function(var) dist_plot(da_data, var)) %>% 
  .[num_vars]
  
# Print resulting plots
print(
  map(seq_along(plots), 
      function(i) {
        plots[[i]] + 
        labs(title = paste0("Distribution of ", names(plots[i])),
             x = names(plots[i]))
      }))
```

### Words:
These will need to be handled like natural language
```{r word-dist}
# Identify potential text cols
chr_vars <- da_data %>% select_if(is.character) %>% names

# Tokenize question data
tokened_data <- 
  da_data %>% 
  select(-url, -title) %>% 
  mutate(id = row_number()) %>% 
  unnest_tokens(output = "word", input = "question_only") %>% 
  anti_join(stop_words %>% filter(lexicon == "SMART"), by = "word") %>% 
  mutate(lemmas = textstem::lemmatize_words(word))

# Eventually get these using tf-idf or similar metrics
boring_lemmas <- c("year", "abby")

# Count lemmas by decade
decade_counts <- 
  tokened_data %>% 
  count(decade, lemmas) %>% 
  filter(!lemmas %in% boring_lemmas) %>%
  arrange(decade, desc(n)) %>% 
  group_by(decade) %>% 
  slice(1:10) %>% 
  ungroup

# Summarize findings at high level
decade_counts %>% 
  mutate(decade = as_factor(as.character(decade)),
         lemmas = as_factor(lemmas) %>% fct_rev) %>% 
  ggplot(aes(x = lemmas, y = n)) +
  geom_col(aes(fill = decade), show.legend = F) +
  scale_fill_manual(values = lanyop_pal("all")(4)) +
  coord_flip() +
  facet_wrap(facets = "decade", scales = "free") +
  labs(title = "Top 10 Lemmas By Decade") +
  theme_lanyop()
```

# Data Cleaning

```{r data-clean}

```


# Data Mining

```{r data-mine}
random_sample <- da_data %>% 
  mutate(row_id = row_number()) %>% 
  filter(row_id %in% sample(x = nrow(da_data), size = 50, replace = F))
```
