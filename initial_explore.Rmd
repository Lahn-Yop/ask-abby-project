---
title: "Ask Abby Analysis"
output:
  html_document:
    theme: readable
    df_print: paged
---

# Setup

```{r setup, results='hide'}
pacman::p_load(tidyverse, tidytext, feather)
source("theme_lanyop.R")
```

The data is scraped from historic 'Dear Abby' column letters.  It's currently on GitHub.  This is how you'd get it straight from the source.
```{r data-pull, eval=FALSE}
data_url <- "https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv"

# Specify column types
da_data <- read_csv(data_url, col_types = list(col_double(),
                                               col_double(),
                                               col_character(),
                                               col_character(),
                                               col_character(),
                                               col_double(),
                                               col_character()))

# Parsing issue fix:  day (one row has extra ,)
da_data <- da_data %>% 
  mutate(day = str_remove(day, ",") %>% as.numeric(.),
         decade = year - (year %% 10))  # Also build decade cat

# Save this as a feather file (in case we want it later)
write_feather(da_data, "dear_abby.feather")
```

We're going to use the data pre-pulled as a feather file.
```{r data-load}
# Data was compressed to zip
unzip(zipfile = here::here("dear_abby.zip"), list = T)

# Unzip and read data
unzip(zipfile = here::here("dear_abby.zip"), exdir = here::here())
da_data <- read_feather("dear_abby.feather")

# Clean up
unlink(x = "dear_abby.feather")
```


# Check Distributions

### Numerics:

We want to see the distribution of each of our numeric variables.  We'll be handling these like numeric categoricals.
```{r num-dist, results='hide', warning=FALSE}
# Identify numeric cols
num_vars <- da_data %>% select_if(is.numeric) %>% names

# Basic distribution plotting helper
dist_plot <- function(df, x){
  
  x <- enquo(x)
  
  ggplot(df) + 
    geom_histogram(aes(!!x), fill = lanyop_cols("blue"), binwidth = 1) +
    theme_lanyop()
}

# Build plots in loop
plots <- da_data %>% 
  map_if(is.numeric, function(var) dist_plot(da_data, var)) %>% 
  .[num_vars]
```

Now we can print all of them at once.
```{r num-plots, results='hold'}
# Print resulting plots
print(
  map(seq_along(plots), 
      function(i) {
        plots[[i]] + 
        labs(title = paste0("Distribution of ", names(plots[i])),
             x = names(plots[i]))
      }))
```

### Text:

The text will need to be handled like natural language.  We'll be use `tidytext` to drill into the question data.
```{r word-dist}
# Tokenize question data into lemmas
tokened_data <- 
  da_data %>% 
  select(-url, -title) %>% 
  mutate(id = row_number()) %>% 
  unnest_tokens(output = "word", input = "question_only") %>% 
  anti_join(stop_words %>% filter(lexicon == "SMART"), by = "word") %>% 
  mutate(lemmas = textstem::lemmatize_words(word))

# Bind tf-idf metric
metric_data <- tokened_data %>% 
  count(id, lemmas) %>% 
  bind_tf_idf(term = "lemmas", document = "id", n = "n" ) %>% 
  group_by(lemmas) %>% 
  summarize(tf_idf = mean(tf_idf),
            total  = sum(n)) %>% 
  ungroup 

# Use tf-idf to get common lemmas
common_lemmas <- metric_data %>% 
  filter(tf_idf < 0.5) %>% 
  arrange(-total) %>% 
  pull(lemmas)

# Count all lemmas by decade
decade_counts <- 
  tokened_data %>% 
  count(decade, lemmas) %>% 
  filter(!lemmas %in% c("abby", "year")) %>%
  arrange(decade, desc(n))
```

Let's see which lemmas are the most used in letters in each decade.
```{r dec-1}
# Top 10 by decade
decade_counts %>% 
  group_by(decade) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(decade = as_factor(as.character(decade)),
         lemmas = as_factor(lemmas) %>% fct_rev) %>% 
  ggplot(aes(x = lemmas, y = n)) +
  geom_col(aes(fill = decade), show.legend = F) +
  scale_fill_manual(values = lanyop_pal("all")(4)) +
  coord_flip() +
  facet_wrap(facets = "decade", scales = "free") +
  labs(title = "Top 10 Lemmas By Decade") +
  theme_lanyop()
```

Now let's use the `common_lemmas` we found to filter down to a more unique top ten for each decade.
```{r dec-2}
# Top 10 by decade (filtered)
decade_counts %>% 
  filter(!lemmas %in% common_lemmas[1:100]) %>%
  group_by(decade) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(decade = as_factor(as.character(decade)),
         lemmas = as_factor(lemmas) %>% fct_rev) %>% 
  ggplot(aes(x = lemmas, y = n)) +
  geom_col(aes(fill = decade), show.legend = F) +
  scale_fill_manual(values = lanyop_pal("all")(4)) +
  coord_flip() +
  facet_wrap(facets = "decade", scales = "free") +
  labs(title = "Top 10 Lemmas By Decade (filtered for uniqueness)", 
       subtitle = "Group avg tf-idf > 0.5)") +
  theme_lanyop()
```

# Data Cleaning

```{r data-clean}

```


# Data Mining

```{r data-mine}
random_sample <- da_data %>% 
  mutate(row_id = row_number()) %>% 
  filter(row_id %in% sample(x = nrow(da_data), size = 50, replace = F))
```
